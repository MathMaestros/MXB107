%!TEX TS-program = xelatex
%!TEX options = -aux-directory=Debug -shell-escape -file-line-error -interaction=nonstopmode -halt-on-error -synctex=1 "%DOC%"
\documentclass{article}
\input{LaTeX-Submodule/template.tex}

% Additional packages & macros

% Header and footer
\newcommand{\unitName}{Introduction to Statistical Modelling}
\newcommand{\unitTime}{Semester 2, 2022}
\newcommand{\unitCoordinator}{Dr Gentry White}
\newcommand{\documentAuthors}{Tarang Janawalkar}

\fancyhead[L]{\unitName}
\fancyhead[R]{\leftmark}
\fancyfoot[C]{\thepage}

% Copyright
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
    imagewidth={5em},
    hyphenation={raggedright}
]{doclicense}

\date{}

\begin{document}
%
\begin{titlepage}
    \vspace*{\fill}
    \begin{center}
        \LARGE{\textbf{\unitName}} \\[0.1in]
        \normalsize{\unitTime} \\[0.2in]
        \normalsize\textit{\unitCoordinator} \\[0.2in]
        \documentAuthors
    \end{center}
    \vspace*{\fill}
    \doclicenseThis
    \thispagestyle{empty}
\end{titlepage}
\newpage
%
\tableofcontents
\newpage
%
\section{Introduction}
Statistics is a field of mathematics that deals with data.
It includes the study of summarising data, constructing probabilistic models, estimating
parameters, and making statistical inferences.

Statistical modelling includes asking questions, obtaining data and determining a mathematical model.
\subsection{Elements of Statistical Modelling}
\subsubsection{Data}
Data is a collection of numbers that describes some characteristic that can be ranked, counted, or measured.
\subsubsection{Collecting information}
Statistical modelling relies upon reliably sourced data. When collecting data,
we must consider
\begin{itemize}
    \item what questions are we trying to answer,
    \item what information is needed to answer these questions,
    \item what is the best source for that information
\end{itemize}
\subsubsection{Randomness}
We must be aware that everything is different and that
randomness introduces uncertainty in data.
Random events are events whose exact outcome cannot be predicted.
We can assume that all variation in the world is observed due to randomness.
\subsubsection{Probability}
Probability is a mathematical construct for dealing with randomness and uncertainty.
\subsection{Experimental Units and Populations}
\begin{definition}[Experimental unit]
    An \textbf{experimental unit} is an individual that generates information for the data collection process.
    Careful consideration of what constitutes an \linebreak experimental unit must be made to ensure that it aligns with the questions of interest.
\end{definition}
\subsubsection{Sample vs. Population}
\begin{definition}[Population]
    We might have questions about a very large collection of things called a \textbf{population}.

    A dataset collected from a population is called a census.
\end{definition}
As it is not feasible to collect data from an entire population,
we must use a sample of the population.
\begin{definition}[Sample]
    A \textbf{sample} is a subset of a population that is representative of the population, in some cases a random sample is sufficient.
\end{definition}
\begin{definition}[Random sample]
    A \textbf{random sample} is one where the sample members are selected from the population by chance.
\end{definition}
\subsection{Types of Data}
\subsubsection{Univariate, Bivariate, and Multivariate}
Data can be described in terms of dimension, that is, how many measurements were collected from each experimental unit.
By collecting multiple measurements from each experimental unit, we can ask questions about the relationship between the measurements.
\begin{itemize}
    \item When a single measurement is collected, the resulting dataset is \textbf{univariate}.
    \item If two measurements are collected, the dataset is \textbf{bivariate}.
    \item If more than two measurements are collected, the dataset is \textbf{multivariate}.
\end{itemize}
\subsubsection{Experimental vs. Observational Data}
Data sets that have been collected without any specific analyses or modelling in mind are called \textbf{observational data}.
By contrast, when a collection procedure is specifically designed to obtain data with a specific intent,
i.e., a laboratory test, the data is called \textbf{experimental data}.

Observational data may contain biases that limit its usefulness and bias any modelling or analysis results.
\subsubsection{Quantitative Data}
Quantitative data is data that is expressed numerically.
This data can be classified as \textit{discrete}, \textit{continuous}, or \textit{ordinal}.
\begin{itemize}
    \item Count data is classified as discrete, i.e., integer values or finite sets
          of real values.
    \item Continuous data is a measurement on a continuum or a measure that can be subdivided infinitely,
          i.e., time and lengths.
    \item Ordinal data is data where the order or ranking of values (discrete or continuous) is important.
\end{itemize}
When data is not ordinal, it is called \textbf{nominal} data.
\subsubsection{Qualitative Data}
Qualitative (categorical) data is data where the variable of interest is
membership to a group or category.
\subsection{Summarising and Describing Data}
\subsubsection{Tables}
Tables are the most immediate way of summarising a data set.
We might organise data in a table with one row for each subject and a column for each measurement.
\subsection{Bar Charts}
Graphical depictions of the data can also be useful but are limited in the number of variables displayed in one picture.

Bar charts are most useful for categorical data where categories are listed on the \(x\)-axis of the plot, and bars for each category are drawn with their heights corresponding to the \textit{counts} for that category.

When the categories are \textbf{ordered} from left to right in descending order counts, the plot is called a \textbf{Pareto plot}.
\subsection{Line Charts}
Line charts illustrate a \textit{trend} of change based on \textbf{two} quantitative variables. Typically line charts display trends over time (or other ordinal variables).

Often trends over time need to be aggregated by plotting the average or median per year to avoid a ``busy'' plot which can sometimes be difficult to read.

While the resulting chart can explain overall trends, they can obscure how much variability or ``noise''
is in the data and may be misleading if the overall trend is obscured by variability.
\subsection{Histograms}
Histograms are a special kind of bar chart that give a visual description of data by ``binning'' or grouping data into data ranges,
then plotting bars with heights equal to the count of the bins' contents \textit{or} the relative proportion of the bins' contents.

Histograms give us a picture of the shape of the data and help identify patterns in the distribution of values.

The binning process is performed by the computer, however in most cases we override the automatic settings and select either the number of
bins, or the width of each bin.
\subsection{Plots, Graphs, and Charts}
\begin{itemize}
    \item A \textbf{chart} is a visual display of data, i.e., a table, a graph, or a diagram
    \item A \textbf{graph} is a diagram showing the relationship between variables, each measured along orthogonal axes.
    \item A \textbf{plot} is used as a synonym for graph but is less precise in its definition; it also sometimes refers specifically to a graph \textit{produced by a computer}.
\end{itemize}
\subsection{Interpreting Graphical Descriptions}
Graphical descriptions of data should ensure that all information about the data is expressed.
\begin{itemize}
    \item The \(x\) and \(y\) axes should be clear in what they are measuring, including any units.
    \item Consider how the graph or chart was made. What choices were made and how might different options change how the graph is perceived.
    \item Does the graph contain any outliers that merit investigation to determine if they are accurate measurements, or if they result from either measurement or recording error.
    \item For Pareto charts and histograms; the \(y\)-axis should measure proportion or density rather than frequency to make comparisons easier.
\end{itemize}
\subsubsection{Centrality}
Histograms are a graphical representation of the distribution or density of observations. Centrality is the degree to which an observation is central to the distribution. Additionally, the data can be multi-modal if there are multiple ``peaks'' or ``centres'' in the distribution.

Altering the number of bins or bin width may reveal the centrality of the observations.
\subsubsection{Skew}
Another characteristic of histograms is the degree to which the distribution is skewed. Skew is the deviation from symmetry about the centre of the data. Skew is either ``right'' skew where the tail of the density or histogram is heavier to the right, or ``left'' skew if otherwise.

This can be observed by looking at how much the left/right tails are stretched in comparison to one another, i.e., the tail to the right of a right skewed chart stretches further on the \(x\)-axis than on the left.
\subsubsection{Trends}
Trends refer to changes in a line chart and are often described as a
constant (first-derivative) pattern of increasing or decreasing values.
\section{Numerical Summaries of Data}
Although graphical summaries are useful for developing a general understanding data,
they are limited to subjective interpretations. To form a precise understanding of the data, we need to use numerical summaries.
Here we must make a distinction between sample and population summaries as measurements may vary
between samples, whereas population summaries are generally constant.
\subsection{Measures of Centrality}
\subsubsection{Mean}
Given a set of \(n\) observations \(x_1,\: x_2,\: \ldots,\: x_n\), the \textbf{arithmetic mean}
or \textbf{average} is defined as
\begin{equation*}
    \frac{1}{n} \sum_{i = 1}^n x_i \equiv \frac{1}{n} \left( x_1 + x_2 + \cdots + x_n \right)
\end{equation*}
If the data is taken from a sample, the sample mean is denoted \(\overline{x}\).
\subsubsection{Median}
A drawback to the mean is that it can be misleading when the data is skewed.
The \textbf{median} is the middle value of a set of \(n\) observations when arranged
from largest to smallest.

If \(n\) is odd:
\begin{equation*}
    \text{median} = x^{\left( \frac{n + 1}{2} \right)}
\end{equation*}
or the \(\left( n + 1 \right) / 2\)th value of the sorted list.
If \(n\) is even, the median is the :
\begin{equation*}
    \text{median} = \frac{x^{\left( \frac{n}{2} \right)} + x^{\left( \frac{n}{2} + 1 \right)}}{2}
\end{equation*}
\subsubsection{Mode}
Given discrete data, the mode is defined as the most common value in a set of observations.
\subsubsection{Population Mean}
The mean of a finite population is computed in the same way as the mean of a sample,
but the population mean is denoted by \(\mu\).
\subsection{Measures of Dispersion}
Dispersion refers to how much variation there is in a set of observations.
\subsubsection{Range}
Given a set of observations that are ordered such that
\begin{equation*}
    x^{\left( 1 \right)} \leq x^{\left( 2 \right)} \leq \cdots \leq x^{\left( n \right)}
\end{equation*}
the range is defined as
\begin{equation*}
    x^{\left( n \right)} - x^{\left( 1 \right)}.
\end{equation*}
\subsubsection{Variance}
The variance is the average of the squared deviations from the mean.
\begin{itemize}
    \item Given the observations \(x_1,\: x_2,\: \ldots,\: x_N\), from a population of size \(N\) with mean \(\mu\),
          the \textbf{population variance} is defined as
          \begin{equation*}
              \sigma^2 = \frac{1}{N} \sum_{i = 1}^N (x_i - \mu)^2.
          \end{equation*}
    \item Given the observations \(x_1,\: x_2,\: \ldots,\: x_n\), from a sample of size \(n\) with mean \(\overline{x}\),
          the \textbf{sample variance} is defined as
          \begin{equation*}
              s^2 = \frac{1}{n - 1} \sum_{i = 1}^n (x_i - \overline{x})^2.
          \end{equation*}
\end{itemize}
The \textbf{population variance} is given by
\subsubsection{Standard Deviation}
The standard deviation is the square root of the variance. This
is conceptually easier to understand as it has the same units are the data.
\begin{itemize}
    \item The \textbf{population standard deviation} is defined as
          \begin{equation*}
              \sigma = \sqrt{\sigma^2}.
          \end{equation*}
    \item The \textbf{sample standard deviation} is defined as
          \begin{equation*}
              s = \sqrt{s^2}.
          \end{equation*}
\end{itemize}
\begin{theorem}[Chebyshev's Theorem]
    Given a set of \(n\) observations, at least
    \begin{equation*}
        1 - \frac{1}{k^2}
    \end{equation*}
    of them are within \(k\) standard deviations of the mean,
    where \(k \geq 1\).
    Formally,
    \begin{equation*}
        \frac{\#\left\{ x \vert \overline{x} - ks < x < \overline{x} + k s \right\}}{n} \geq 1 - \frac{1}{k^2}
    \end{equation*}
\end{theorem}
\begin{theorem}[Empirical Rule]
    If a histogram of the data is approximately unimodal and symmetric, then,
    \begin{itemize}
        \item 68\% of the data falls within \textbf{one} standard deviation of the mean
        \item 95\% of the data falls within \textbf{two} standard deviations of the mean
        \item 99\% of the data falls within \textbf{three} standard deviations of the mean
    \end{itemize}
\end{theorem}
Often the standard deviation cannot be computed directly, but can be approximated
using the Empirical rule. Here we assume that
\begin{equation*}
    \text{range} \approx 4 s
\end{equation*}
so that
\begin{equation*}
    s = \frac{\text{range}}{4}.
\end{equation*}
\subsection{Skew}
The \textbf{skew} describes the asymmetry of the distribution.
For a finite population of size \(N\), the \textbf{population skew} is defined as
\begin{equation*}
    \frac{1}{N} \sum_{i = 1}^N \left( \frac{x_i - \mu}{\sigma} \right)^3
\end{equation*}
For a sample of size \(n\), the \textbf{sample skew} is defined as
\begin{equation*}
    \frac{\frac{1}{n} \sum_{i = 1}^n \left( x_i - \overline{x} \right)^3}{\left( \sqrt{\frac{1}{n - 1} \sum_{i = 1}^n \left( x_i - \overline{x} \right)^2} \right)^3}
\end{equation*}
\begin{itemize}
    \item When the skew is \textbf{positive}, the data is \textbf{right-skewed} and the ``tail'' of the distribution is \textbf{longer on the right}
    \item When the skew is \textbf{negative}, the data is \textbf{left-skewed} and the ``tail'' of the distribution is \textbf{longer on the left}
\end{itemize}
\subsection{Measures of Rank}
It is often useful to know the rank or \textit{relative standing} of a value in a set of observations.
This is natural for ordinal data whose ordering has implicit meaning, but it can
also be useful for nominal data as a means of measuring dispersion.
\subsubsection{Z-Score}
The Z-score is a unitless quantity and can be used to make comparisons of relative rank between members of a population.
\begin{equation*}
    Z = \frac{x  - \mu}{\sigma} \quad \text{or} \quad \frac{x - \overline{x}}{s}
\end{equation*}
\subsubsection{Quantiles}
In addition to Z-scores, quantiles can be used to make comparisons of relative ranking between populations,
as well as construct intervals bounding a given proportion of the observations.
For a set of \(n\) observations, \(x_q\) is the \(q\)-th quantile, if \(q\)\% of the observations are less than \(x_q\).
\subsubsection{Inter-Quartile Range}
The inter-quartile range (IQR) is the difference between the 75th and 25th quantiles,
or the range covered by the middle 50\% of data. It is a robust measure of the dispersion of the data,
as it is not affected by extreme values unlike the range or variance.
\subsection{Boxplots}
\subsubsection{Five Number Summary}
The five number summary is set of measurements that indicates the
\begin{itemize}
    \item minimum value
    \item 25\% quartile
    \item median
    \item 75\% quartile
    \item maximum value
\end{itemize}
A boxplot is a graphical display of the five number summary.
It is a plot of the values of the data mapped to the \(y\)-axis.

Using the \mintinline{R}{ggplot2} package, the function \mintinline{R}{geom_boxplot()} draws
a box encompassing the IQR with a horizontal line indicating the median.
Vertical lines extend 1.5 times the IQR above and below the box. The points not within
the ends of the vertical lines are also plotted to indicate outliers.
\subsubsection{Outliers}
Outliers are extreme observations that fall outside some
interval defined either by quantiles (above 95\% or below 5\% quantiles) or in terms of the Empirical rule
(outside two standard deviations from the mean).
They should be investigated to determine if they are errors or naturally occurring
extreme values.
\section{Bivariate Data}
Data in two dimensions is often used to describe relationships between two variables.
\subsection{Bivariate Categorical Data}
Bivariate categorical data is a dataset with two qualitative or categorical variables that have
a relationship we want to summarise. This can be done using contingency tables or side-by-side (or stacked) bar charts.
\subsubsection{Contingency Tables}
Contingency tables or crosstabs are tabular representations of the frequency of occurrence of pairs of values.
The categories for each variable are assigned to an axis of the table so that each cell
represents the frequency of occurrence of a pair of categories, one from each variable.
\subsubsection{Bar Plots}
Often the data is presented more effectively as a stacked bar chart or side-by-side bar chart.
Here the counts for each pair of categories are plotted on the same axis, and
stacked on top of one another to display relative proportion, or side-by-side if
too busy.
\subsection{Bivariate Quantitative Data}
Bivariate quantitative data is a dataset with one qualitative variable and one quantitative variable.
This can be represented as a table, or through various charts, by comparing charts side-by-side for each category.
\subsection{Scatter Plots}
When both variables are quantitative, the data can be represented as a scatter plot with
each variable assigned to an axis and the points plotted on the axes.
\subsubsection{Covariance and Correlation Coefficients}
For such data, the covariance is the measure of the linear correlation between the variables.
For variables \(x\) and \(y\),
\begin{equation*}
    s_{xy} = \frac{\sum_{i = 1}^n \left( x_i - \overline{x} \right) \left( y_i - \overline{y} \right)}{n - 1}.
\end{equation*}
Note that when \(x = y\), the formula simplifies to the sample variance of \(x\).
The covariance has the following characterstics:
\begin{itemize}
    \item \(s_{xy} > 0\): As \(x\) increases, \(y\) also increases.
    \item \(s_{xy} < 0\): As \(x\) increases, \(y\) decreases.
    \item \(s_{xy} \approx 0\): No relationship between \(x\) and \(y\).
\end{itemize}
Although the covariance is a useful tool to measure relationships, it is
only generalisable in terms of its sign. Thus, if we want to compare across
data sets, we need to use the correlation coefficient.
\begin{equation*}
    r_{xy} = \frac{s_{xy}}{s_x s_y}
\end{equation*}
The \textbf{correlation coefficient} is a measure of the strength of the relationship between the variables.
It is a scale-free and unitless measure bounded between \(-1\) and \(1\) and
has the same characteristics as the covariance.

Note that a correlation coefficient of \(0\)
indicates \textbf{no linear relationship} between the variables, and not necessarily indicative of \textbf{no relationship}.
\subsection{Regression and Least Squares}
In addition to the numerical summaries above, a regression or least squares line of
best fit provides both a graphical and numerical summary of the relationship between the variables.
A linear relationship between two variables \(x\) and \(y\) is defined as
\begin{equation*}
    y = a + b x.
\end{equation*}
The least squares best fit determines the coefficients \(a\) and \(b\) that minimise the sum of the squares of the residuals
(errors) between \(y\) and the line \(\hat{y} = a + b x\). Mathematically,
\begin{equation*}
    \min_{a,\: b} \sum_{i = 1}^n \left( y_i - \hat{y}\left( x_i \right) \right)^2.
\end{equation*}
The coefficients can be summarised by the formula
\begin{align*}
    b & = r \frac{s_y}{s_x} = \frac{s_{xy}}{s_x^2} \\
    a & = \overline{y} - b \overline{x}.
\end{align*}
\section{Probability}
\subsection{Experiments, Events, Sample Space}
\begin{definition}[Experiment]
    An experiment is a situation that produces some observable phenomena where the outcome is impossible to predict with certainty.
\end{definition}
\begin{definition}[Simple event]
    A simple event is the outcome of a single repetition of an experiment.
\end{definition}
\begin{definition}[Event]
    An event is a collection of simple events, or the outcome of multiple repetitions of an event.
    Events are often denoted by a capital letter.
\end{definition}
\begin{definition}[Mutually exclusive]
    Events are mutually exclusive if the occurrence of one event precludes the occurrence of another.
    In other words, if one event occurs, the other event cannot occur.
\end{definition}
\begin{definition}[Sample space]
    A sample space is the set of possible simple events, or all outcomes of an experiment.
\end{definition}
\subsection{Probability of Events}
\subsubsection{Probability of an Event}
For a discrete finite sample space, the probability of a simple event is defined as the relative
frequency of an outcome. Given the simple event \(A\),
\begin{equation*}
    \Pr{\left( A \right)} = \lim_{n \to \infty} \frac{I_A}{n}
\end{equation*}
where \(I_A\) is a function that evaluates to 1 if \(A\) occurs and 0 otherwise.

The probabilities of events must satisfy the following conditions:
\begin{itemize}
    \item \(0 \leq \Pr{\left( A \right)} \leq 1\), where \(A\) is a simple event.
    \item The sum of the probabilities over the sample space is 1.
\end{itemize}
If an event \(A\) consists of a collection of simple events and each outcome is
equally likely, then we can calculate the probability of an event as
\begin{equation*}
    \Pr{\left( A \right)} = \frac{\text{number of ways that \(A\) can occur}}{\text{total number of outcomes}}
\end{equation*}
\subsubsection{Probability of an Event in a Sample Space}
Given the continuous sample space \(S\), the event \(A\) can be defined as a subset of \(S\), \(A \subseteq S\).
The definition of the probability of event \(A\) can be written as
\begin{equation*}
    \Pr{\left( A \right)} = \frac{\text{the area of region-\(A\)}}{\text{the area of region-\(S\)}}
\end{equation*}
As this probability is a ratio, it can be standardised so that the area of \(S\) is 1.
Thus \(\Pr{\left( A \right)}\) is the area of region-\(A\).
\subsubsection{Probability of the Complement}
The complement of an event \(A\) is every event not in \(A\), and is denoted as
\(A^c\) or \(\overline{A}\). Since the total probability for the sample space is 1, then
the probability of \(A^c\) is:
\begin{equation*}
    \Pr{\left( A^c \right)} = 1 - \Pr{\left( A \right)}
\end{equation*}
This is true because \(A \cup A^c = S\) and \(\Pr{\left( S \right)} = 1\).
\subsubsection{Probability of Subsets}
If \(B \subset A\), then \(\Pr{\left( B \right)} \leq \Pr{\left( A \right)}\).
\subsubsection{Addition Law}
\begin{equation*}
    \Pr{\left( A \cup B \right)} = \Pr{\left( A \right)} + \Pr{\left( B \right)} - \Pr{\left( A \cap B \right)}.
\end{equation*}
and for disjoint events:
\begin{equation*}
    \Pr{\left( A \cup B \right)} = \Pr{\left( A \right)} + \Pr{\left( B \right)}.
\end{equation*}
so that the intersection \(\Pr{\left( A \cap B \right)} = 0\).
\subsection{Conditional Probability}
Conditional probability is the probability of an event \(A\) given another event \(B\) occurs.

If \(A \cap B = \emptyset\), then \(\Pr{\left( A \cap B \right)} = 0\). Thus if we know that \(B\) has
occurred, then we know that \(A\) cannot occur:
\begin{equation*}
    \Pr{\left( A \,\vert\, B \right)} = 0.
\end{equation*}
Therefore if \(A \cap B \neq \emptyset\), then \(\Pr{\left( A \cap B \right)} \neq 0\).
If \(B \neq \emptyset\), then the conditional probability of \(A\) given \(B\) is given by:
\begin{equation*}
    \Pr{\left( A \,\vert\, B \right)} = \frac{\Pr{\left( A \cap B \right)}}{\Pr{\left( B \right)}}.
\end{equation*}
Note that when \(\Pr{\left( A \right)} > \Pr{\left( B \right)}\), \(\Pr{\left( A \,\vert\, B \right)} > \Pr{\left( B \,\vert\, A \right)}\).
\subsubsection{Independence}
Independence can be defined in terms of conditional probability.
\(A\) and \(B\) are independent events if
\begin{equation*}
    \Pr{\left( A \,\vert\, B \right)} = \Pr{\left( A \right)}.
\end{equation*}
This leads to the multiplication rule for independent events:
\begin{equation*}
    \Pr{\left( A \cup B \right)} = \Pr{\left( A \right)} \Pr{\left( B \right)}.
\end{equation*}
\subsection{Bayes' Rules}
\subsubsection{Law of Total Probability}
By partitioning the sample space \(S\) into a collection of disjoint events \(B_1,\; B_2,\; \dots,\; B_n\),
such that \(\bigcup_{i = 1}^n B_i = S\), we have
\begin{equation*}
    \Pr{\left( A \right)} = \sum_{i = 1}^n \Pr{\left( A \,\vert\, B_i \right)}\Pr{\left( B_i \right)}
\end{equation*}
where \(\Pr{\left( A \,\vert\, B_i \right)} \Pr{\left( B_i \right)} = \Pr{\left( A \cap B_i \right)}\).
Given the probability for \(A\) given \(B\), the probability of the reverse direction is given by
\begin{equation*}
    \Pr{\left( B \,\vert\, A \right)} = \frac{\Pr{\left( A \,\vert\, B \right)}\Pr{\left( B \right)}}{\Pr{\left( A \right)}}
\end{equation*}
this is known as \textbf{Bayes' Theorem}. Using the law of total probability, we can express this as
\begin{equation*}
    \Pr{\left( B \,\vert\, A \right)} = \frac{\Pr{\left( A \,\vert\, B \right)}\Pr{\left( B \right)}}{\sum_{i = 1}^n \Pr{\left( A \,\vert\, B_i \right)}\Pr{\left( B_i \right)}}.
\end{equation*}
\section{Probability Distributions}
\subsection{Random Variables}
A random variable is a variable whose value is the result of an experiment or random trial,
where the value is not known before the trial with certainty.
\subsection{Discrete Random Variables}
\begin{definition}[Discrete random variables]
    A discrete random variable takes on values in \(\N_0\), where the random variables arises from
    counting processes.
\end{definition}
\subsubsection{Probability Mass Function}
The probability mass function (PMF) of a discrete random variable \(X\) is a function \(p\left( x \right)\) that
maps values from the sample space of \(X\) onto the interval \(\interval{0}{1}\).
\begin{equation*}
    p\left( x \right) = \Pr{\left( X = x \right)}.
\end{equation*}
This function is constrained by the following properties:
\begin{itemize}
    \item \(p\left( x \right) = 0\) if \(x \notin X\).
    \item \(p\left( x \right) \in \interval{0}{1}\) if \(x \in X\).
    \item \(\sum_{\forall x \in X} p\left( x \right) = 1\).
\end{itemize}
\subsubsection{Cumulative Mass Function}
The cumulative mass function (CMF) is defined
\begin{equation*}
    F\left( x \right) = \Pr{\left( X \leq x \right)} = \sum_{-\infty}^x p\left( x \right)
\end{equation*}
and the probabilities for events can be defined using the CMF\@:
\begin{equation*}
    \Pr{\left( a < X \leq b \right)} = \sum_{x = a + 1}^b p\left( x \right) = F\left( b \right) - F\left( a \right).
\end{equation*}
\subsubsection{Expectation}
The expectation (expected value) of a random variable \(X\) with a PMF is given by:
\begin{equation*}
    \E{\left( X \right)} = \sum_{\forall x \in X} x p\left( x \right)
\end{equation*}
where the expectation is often denoted \(\mu\). As the expectation is a weighted average of all possible values in \(X\),
we can extend this definition to any function of \(X\):
\begin{equation*}
    \E{\left( h\left( X \right) \right)} = \sum_{\forall x \in X} h\left( x \right) p\left( x \right).
\end{equation*}
\subsubsection{Median and Mode}
The median \(m\) of a discrete random variable \(X\) is defined:
\begin{equation*}
    m \in X : \Pr{\left( X \leq m \right)} \geq \frac{1}{2} \land \Pr{\left( X \geq m \right)} \geq \frac{1}{2}.
\end{equation*}
Note that \(\Pr{\left( X \leq x \right)} = \sum_{-\infty}^x p\left( x \right)\) and \(\Pr{\left( X \geq x \right)} = \sum_x^\infty p\left( x \right)\).

The mode of a discrete random variable \(X\) is defined:
\begin{equation*}
    \max_{x \in X} p\left( x \right).
\end{equation*}
\subsubsection{Variance}
The variance of a random variable \(X\) is defined using the mean:
\begin{align*}
    \Var{\left( X \right)} & = \E{\left( \left( X - \mu \right)^2 \right)}                        \\
                           & = \E{\left( X^2 \right)} - 2 \mu \E{\left( X \right)} + \mu^2        \\
                           & = \E{\left( X^2 \right)} - \E{\left( X \right)}^2                    \\
                           & = \sum_{\forall x \in X} \left( x - \mu \right)^2 p\left( x \right).
\end{align*}
\subsection{Continuous Random Variables}
\begin{definition}[Continuous random variables]
    A continuous random variable takes on values in \(R\), where values lie on a continuum.
\end{definition}
\subsubsection{Probability Density Function}
The probability density function (PDF) of a continuous random variable \(X\) is a function \(f\left( x \right)\) that
describes the density of possible values for a continuous random variable. Note that it does not define the probability
of a specific value.

For a continuous random variable \(X\):
\begin{itemize}
    \item \(\Pr{\left( X = x \right)} = 0\) and \(f\left( x \right) \neq \Pr{\left( X = x \right)}\).
    \item \(\forall x \in X : f\left( x \right) \geq 0\).
    \item \(\int_{-\infty}^\infty f\left( u \right) \odif{u} = 1\).
\end{itemize}
As the probability of a single value is zero, we can instead quantify the probability of a range of values:
\begin{equation*}
    \Pr{\left( a \leq x \leq b \right)} = \int_{a}^{b} f\left( x \right) \odif{x}
\end{equation*}
where \(a \leq b\).
\subsubsection{Cumulative Density Function}
The cumulative density function (CDF) is defined
\begin{equation*}
    F\left( x \right) = \Pr{\left( X \leq x \right)} = \sum_{-\infty}^x f\left( u \right) \odif{u}
\end{equation*}
so that
\begin{equation*}
    \Pr{\left( a \leq X \leq b \right)} = F\left( b \right) - F\left( a \right).
\end{equation*}
In the continuous case, the PDF and CDF are related through the following differential equation:
\begin{equation*}
    \odv{F}{x} = f\left( x \right).
\end{equation*}
\subsubsection{Median and Mode}
The median \(m\) of a continuous random variable \(X\) is defined:
\begin{equation*}
    m : \int_{-\infty}^m f\left( u \right) \odif{u} = \frac{1}{2}
\end{equation*}
and the mode is defined:
\begin{equation*}
    \max_{x \in X} f\left( x \right) \quad \text{or} \quad m:\odv{f\left( y \right)}{y} = 0.
\end{equation*}
\subsubsection{Expectation}
The expectation of a continuous random variable \(X\) is defined as
\begin{equation*}
    \mu = \E{\left( X \right)} = \int_{-\infty}^\infty x f\left( x \right) \odif{x}
\end{equation*}
\subsubsection{Variance}
The variance of a continuous random variable \(X\) is defined:
\begin{equation*}
    \Var{\left( X \right)} = \E{\left( X^2 \right)} - \E{\left( X \right)}^2 = \int_{-\infty}^\infty \left( x - \mu \right)^2 f\left( x \right)^2 f\left( x \right) \odif{x}.
\end{equation*}
\subsection{Probability Distributions}
\subsubsection{Bernoulli Distribution}
A Bernoulli (or binary) distribution describes the probability distribution of a Boolean-valued
outcome, i.e., success (1) or failure (0).

A discrete random variable \(X\) with a Bernoulli distribution is denoted
\begin{equation*}
    X \sim \operatorname{Bernoulli}{\left( p \right)}
\end{equation*}
with
\begin{align*}
    \Pr{\left( X = x \right)}    & = \begin{cases}
                                         1 - p & x = 0 \\
                                         p     & x = 1
                                     \end{cases}                    \\
                                 & = p^x \left( 1 - p \right)^{1 - x} \\
    \Pr{\left( X \leq x \right)} & = \begin{cases}
                                         0     & x < 0        \\
                                         1 - p & 0 \leq x < 1 \\
                                         1     & k \geq 1
                                     \end{cases}
\end{align*}
for a probability \(p \in \interval{0}{1}\) and outcomes \(x \in \left\{ 0,\: 1 \right\}\).
We can also summarise the following:
\begin{align*}
    \E{\left( X \right)}   & = p                      \\
    \Var{\left( X \right)} & = p \left( 1 - p \right)
\end{align*}
where \(\left( 1 - p \right)\) is sometimes denoted as \(q\).
\subsubsection{Binomial Distribution}
A binomial distribution describes the probability distribution of the number of successes
for \(n\) independent trials with the same probability of success \(p\).

A discrete random variable \(X\) with a binomial distribution is denoted
\begin{equation*}
    X \sim \operatorname{Binomial}{\left( n,\: p \right)}
\end{equation*}
with
\begin{align*}
    \Pr{\left( X = x \right)}    & = \dbinom{n}{x} p^x \left( 1 - p \right)^{n - x}                \\
    \Pr{\left( X \leq x \right)} & = \sum_{u = 0}^x \dbinom{n}{u} p^u \left( 1 - p \right)^{n - u}
\end{align*}
for number of successes \(x \in \left\{ 0,\: 1,\: \dots,\: n \right\}\).

Here each individual trial is a Bernoulli trial, so that \(X\) can be written as the sum of
\(n\) \textit{independent and identically distributed} (iid) Bernoulli random variables, \(Y_1,\: Y_2,\: \dots,\: Y_n\).
\begin{align*}
    X & = Y_1 + Y_2 + \cdots + Y_n, & Y_i & \overset{\mathrm{iid}}{\sim} \operatorname{Bernoulli}{\left( p \right)} : \forall i \in \left\{ 1,\: 2,\: \dots,\: n \right\}.
\end{align*}
We can then summarise the following:
\begin{align*}
    \E{\left( X \right)}   & = np                      \\
    \Var{\left( X \right)} & = np \left( 1 - p \right)
\end{align*}
\subsubsection{Poisson Distribution}
A Poisson distribution describes the probability distribution of the number of events \(N\) which occur over a fixed interval of time \(\lambda\).

A discrete random variable \(N\) with a Poisson distribution is denoted
\begin{equation*}
    N \sim \operatorname{Poisson}{\left( \lambda \right)}
\end{equation*}
with
\begin{align*}
    \Pr{\left( N = n \right)}    & = \frac{\lambda^n e^{-\lambda}}{n!}                \\
    \Pr{\left( N \leq n \right)} & = e^{-\lambda} \sum_{u = 0}^n \frac{\lambda^u}{u!}
\end{align*}
for number of events \(n \geq 0\).
We can also summarise the following:
\begin{align*}
    \E{\left( N \right)}   & = \lambda \\
    \Var{\left( N \right)} & = \lambda
\end{align*}
The Poisson PMF can be defined in terms of the Binomial PMF as \(n \to \infty\) and \(p \to 0\).
Let \(\lambda = np\), then
\begin{align*}
    p\left( x \right) & = \frac{n!}{x! \left( n - x \right)!} p^x \left( 1 - p \right)^{n - x}                                                                                 \\
                      & = \frac{n!}{x! \left( n - x \right)!} \left( \frac{\lambda}{n} \right)^x \left( 1 - \frac{\lambda}{n} \right)^{n - x}                                  \\
                      & = \frac{\lambda^x}{x!} \frac{n!}{\left( n - x \right)!} \frac{1}{n^x} \left( 1 - \frac{\lambda}{n} \right)^n \left( 1 - \frac{\lambda}{n} \right)^{-x} \\
\end{align*}
The limit of \(\frac{n!}{\left( n - x \right)!} \frac{1}{n^x}\) is \(1\):
\begin{align*}
    \lim_{n \to \infty} \frac{n!}{\left( n - x \right)!} \frac{1}{n^x} & = \lim_{n \to \infty} \frac{n\left( n - 1 \right) \left( n - 2 \right) \cdots \left( n - x + 1 \right)}{n^x}                                            \\
                                                                       & = \lim_{n \to \infty} \left( \frac{n}{n} \right)\left( \frac{n - 1}{n} \right) \left( \frac{n - 2}{n} \right) \cdots \left( \frac{n - x + 1}{n} \right) \\
                                                                       & = 1
\end{align*}
The term \(\left( 1 - \frac{\lambda}{n} \right)^n\) approaches \(e^{-\lambda}\), using the substitution \(u = -\frac{n}{\lambda}\):
\begin{align*}
    \lim_{n \to \infty} \left( 1 - \frac{\lambda}{n} \right)^n & = \lim_{n \to \infty} \left( 1 + \frac{1}{u} \right)^{u \left( -\lambda \right)} \\
                                                               & = e^{-\lambda}
\end{align*}
Finally, the remaining term also evaluates to 1:
\begin{equation*}
    \lim_{n \to \infty} \left( 1 - \frac{\lambda}{n} \right)^{-x} = 1
\end{equation*}
Therefore by gathering the above equations, we can write the Poisson PMF as:
\begin{align*}
    p\left( x \right) & = \lim_{n \to \infty} \frac{\lambda^x}{x!} \frac{n!}{\left( n - x \right)!} \frac{1}{n^x} \left( 1 - \frac{\lambda}{n} \right)^n \left( 1 - \frac{\lambda}{n} \right)^{-x} \\
                      & = \frac{\lambda^x e^{-\lambda}}{x!}
\end{align*}
\subsubsection{Uniform Distribution}
A continuous uniform distribution describes the probability distribution of an outcome within some
interval, where the probability of an outcome in one interval is the same as all other intervals of the same length.

A continuous random variable \(X\) with a continuous uniform distribution is denoted
\begin{equation*}
    X \sim \operatorname{Uniform}{\left( a,\: b \right)}
\end{equation*}
with
\begin{align*}
    f\left( x \right) & = \frac{1}{b - a}     \\
    F\left( x \right) & = \frac{x - a}{b - a}
\end{align*}
for outcomes \(a < x < b\).
We can also summarise the following:
\begin{align*}
    \E{\left( X \right)}   & = \frac{a + b}{2}                   \\
    \Var{\left( X \right)} & = \frac{\left( b - a \right)^2}{12} \\
    m                      & = \frac{a + b}{2}
\end{align*}
\subsubsection{Exponential Distribution}
An exponential distribution describes the probability distribution of the time between events with rate \(\eta\).

A continuous random variable \(T\) with an exponential distribution is denoted
\begin{equation*}
    T \sim \operatorname{Exp}{\left( \eta \right)}
\end{equation*}
with
\begin{align*}
    f\left( t \right) & = \eta e^{-\eta t} \\
    F\left( t \right) & = 1 - e^{-\eta t}
\end{align*}
for time \(t > 0\).
We can also summarise the following:
\begin{align*}
    \E{\left( T \right)}   & = \frac{1}{\eta}                     \\
    \Var{\left( T \right)} & = \frac{1}{\eta^2}                   \\
    m                      & = \frac{\ln{\left( 2 \right)}}{\eta}
\end{align*}
\begin{proof}
    By considering an event taking longer than \(t\) seconds, we can represent this as nothing happening
    over the interval \(\interval{0}{t}\). Using \(T \sim \operatorname{Exp}{\left( \eta \right)}\) and
    \(N \sim \operatorname{Poisson}{\left( \eta t \right)}\), we have
    \begin{equation*}
        \Pr{\left( T > t \right)} = \Pr{\left( N = 0 \right)} = e^{-\eta t}
    \end{equation*}
    where \(\lambda = \eta t\). The CDF for the exponential distribution is then
    \begin{align*}
        \Pr{\left( T < t \right)} & = 1 - \Pr{\left( T > t \right)} \\
                                  & = 1 - e^{-\eta t}.
    \end{align*}
\end{proof}
\subsubsection{Memoryless Property}
In an exponential distribution with \(T \sim \operatorname{Exp}{\left( \eta \right)}\),
the distribution of the waiting time \(t + s\) until a certain event does not depend on
how much time \(t\) has already passed.
\begin{equation*}
    \Pr{\left( T > s + t \,\vert\, T > t \right)} = \Pr{\left( T > s \right)}.
\end{equation*}
The same property also applies in an Geometric distribution with \(N \sim \operatorname{Geometric}{\left( p \right)}\).
\subsubsection{Normal Distribution}
The normal distribution is used to represent many random situations, in particular, measurements and their errors.
This distribution arises in many statistical problems and can be used to \linebreak approximate other distributions
under certain conditions.

A continuous random variable \(X\) with a normal distribution is denoted
\begin{equation*}
    X \sim \operatorname{N}{\left( \mu,\: \sigma^2 \right)}
\end{equation*}
with
\begin{align*}
    f\left( t \right) & = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{\left( x - \mu \right)^2}{2 \sigma^2}}    \\
    F\left( t \right) & = \frac{1}{2} \left( 1 + \erf{\left( \frac{x - \mu}{\sigma \sqrt{2}} \right)} \right)
\end{align*}
for \(x \in \R\) where \(\erf{\left( z \right)} = \frac{2}{\sqrt{\pi}} \int_0^z e^{-t^2} \odif{t}\) is the error function.
We can also summarise the following:
\begin{align*}
    \E{\left( X \right)}   & = \mu      \\
    \Var{\left( X \right)} & = \sigma^2
\end{align*}
Given the complexity of the analytic expressions for the PDF and CDF of the normal distribution, we often
use software to numerically determine probabilities associated with normal distributions.
\subsection{Standard Normal Distribution}
Given \(X \sim \operatorname{N}{\left( \mu,\: \sigma^2 \right)}\), consider the transformation
\begin{equation*}
    Z = \frac{X - \mu}{\sigma}
\end{equation*}
so that \(Z \sim \operatorname{N}{\left( 0,\: 1 \right)}\). This distribution is called the standard normal distribution.
This allows us to deal with the standard normal distribution regardless of \(\mu\) and \(\sigma\).
\end{document}
