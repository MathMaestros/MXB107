%!TEX TS-program = xelatex
%!TEX options = -aux-directory=Debug -shell-escape -file-line-error -interaction=nonstopmode -halt-on-error -synctex=1 "%DOC%"
\documentclass{article}
\input{LaTeX-Submodule/template.tex}

% Additional packages & macros
\usepackage{mathdots}
\setitemize{leftmargin=*,topsep=1ex,partopsep=0ex,itemsep=-1ex,partopsep=0ex,parsep=1ex}
\setlist{leftmargin=*,topsep=1ex,partopsep=0ex,itemsep=-1ex,partopsep=0ex,parsep=1ex}

\usepackage{changepage} % Modify page width
\usepackage{multicol} % Use multiple columns
\usepackage[explicit]{titlesec} % Modify section heading styles

\titleformat{\section}{\raggedright\normalfont\bfseries}{}{0em}{#1}
\titleformat{\subsection}{\raggedright\normalfont\small\bfseries}{}{0em}{#1}

%% A4 page
\geometry{
a4paper,
margin = 10mm
}

%% Hide horizontal rule
\renewcommand{\headrulewidth}{0pt}
\fancyhead{}

%% Hide page numbers
\pagenumbering{gobble}

%% Multi-columns setup
\setlength\columnsep{4pt}

%% Paragraph setup
\setlength\parindent{0pt}
\setlength\parskip{0pt}

%% Customise section heading styles
% \titleformat*\section{\raggedright\bfseries}

\begin{document}
\begin{multicols}{3}
    \section{Introduction}
    \subsection{Population}
    The entire group we are concerned with.
    \subsection{Sample}
    A representative subset of the population.
    \subsection{Quantitative Data}
    Numerical data.
    Could be nominal (discrete or continuous), or ordinal (ordered).
    \subsection{Qualitative Data}
    Categorical data, e.g.\ colour, model.

\section{Measures of Centrality}
\subsection{Mean}
Given a set of \(n\) observations \(x_1,\: x_2,\: \ldots,\: x_n\), the \textbf{arithmetic mean}
or \textbf{average} is defined as
\begin{equation*}
    \frac{1}{n} \sum_{i = 1}^n x_i
\end{equation*}
The sample mean is denoted \(\overline{x}\).
The population mean is denoted \(\mu\).
\subsection{Median}
A drawback to the mean is that it can be misleading when the data is skewed.
The \textbf{median} is the middle value of a set of \(n\) observations when arranged
from largest to smallest.

If \(n\) is odd:
\begin{equation*}
    \text{median} = x^{\left( \frac{n + 1}{2} \right)}
\end{equation*}
or the \(\left( n + 1 \right) / 2\)th value of the sorted list.
If \(n\) is even, the median is the :
\begin{equation*}
    \text{median} = \frac{x^{\left( \frac{n}{2} \right)} + x^{\left( \frac{n}{2} + 1 \right)}}{2}
\end{equation*}
\subsection{Mode}
Given discrete data, the mode is defined as the most common value in a set of observations.
\section{Measures of Dispersion}
Dispersion refers to how much variation there is in a set of observations.
\subsection{Range}
The range is the difference between the maximum and minimum observation.
\subsection{Variance}
The variance is the average of the squared deviations from the mean.
\begin{itemize}
    \item Given the observations \(x_1,\: x_2,\: \ldots,\: x_N\), from a population of size \(N\) with mean \(\mu\),
          the \textbf{population variance} is defined as
          \begin{equation*}
              \sigma^2 = \frac{1}{N} \sum_{i = 1}^N (x_i - \mu)^2.
          \end{equation*}
    \item Given the observations \(x_1,\: x_2,\: \ldots,\: x_n\), from a sample of size \(n\) with mean \(\overline{x}\),
          the \textbf{sample variance} is defined as
          \begin{equation*}
              s^2 = \frac{1}{n - 1} \sum_{i = 1}^n (x_i - \overline{x})^2.
          \end{equation*}
\end{itemize}
The \textbf{population variance} is given by
\subsection{Standard Deviation}
The standard deviation is the square root of the variance.
The \textbf{population standard deviation} is defined as
\(
              \sigma = \sqrt{\sigma^2}.
              \)
The \textbf{sample standard deviation} is defined as
\(
              s = \sqrt{s^2}.
              \)
\begin{theorem}[Chebyshev's Theorem]
    Given a set of \(n\) observations, at least
    \begin{equation*}
        1 - \frac{1}{k^2}
    \end{equation*}
    of them are within \(k\) standard deviations of the mean,
    where \(k \geq 1\).
\end{theorem}
\begin{theorem}[Empirical Rule]
    If a histogram of the data is approximately unimodal and symmetric, then,
    \begin{itemize}
        \item 68\% of the data falls within \textbf{one} standard deviation of the mean
        \item 95\% of the data falls within \textbf{two} standard deviations of the mean
        \item 99\% of the data falls within \textbf{three} standard deviations of the mean
    \end{itemize}
\end{theorem}
Often the standard deviation cannot be computed directly, but can be approximated
using the Empirical rule. Here we assume that
\begin{equation*}
    \text{range} \approx 4 s
\end{equation*}
so that
\begin{equation*}
    s = \frac{\text{range}}{4}.
\end{equation*}
\section{Skew}
The \textbf{skew} describes the asymmetry of the distribution.
For a finite population of size \(N\), the \textbf{population skew} is defined as
\begin{equation*}
    \frac{1}{N} \sum_{i = 1}^N \left( \frac{x_i - \mu}{\sigma} \right)^3
\end{equation*}
For a sample of size \(n\), the \textbf{sample skew} is defined as
\begin{equation*}
    \frac{\frac{1}{n} \sum_{i = 1}^n \left( x_i - \overline{x} \right)^3}{\left( \sqrt{\frac{1}{n - 1} \sum_{i = 1}^n \left( x_i - \overline{x} \right)^2} \right)^3}
\end{equation*}
\begin{itemize}
    \item When the skew is \textbf{positive}, the data is \textbf{right-skewed} and the ``tail'' of the distribution is \textbf{longer on the right}
    \item When the skew is \textbf{negative}, the data is \textbf{left-skewed} and the ``tail'' of the distribution is \textbf{longer on the left}
\end{itemize}
\section{Measures of Rank}
It is often useful to know the rank or \textit{relative standing} of a value in a set of observations.
This is natural for ordinal data whose ordering has implicit meaning, but it can
also be useful for nominal data as a means of measuring dispersion.
\subsection{Z-Score}
The Z-score is a unitless quantity and can be used to make comparisons of relative rank between members of a population.
\begin{equation*}
    Z = \frac{x  - \mu}{\sigma} \quad \text{or} \quad \frac{x - \overline{x}}{s}
\end{equation*}
\subsection{Quantiles}
In addition to Z-scores, quantiles can be used to make comparisons of relative ranking between populations,
as well as construct intervals bounding a given proportion of the observations.
For a set of \(n\) observations, \(x_q\) is the \(q\)-th quantile, if \(q\)\% of the observations are less than \(x_q\).
\subsection{Inter-Quartile Range}
The inter-quartile range (IQR) is the difference between the 75th and 25th quantiles,
or the range covered by the middle 50\% of data. It is a robust measure of the dispersion of the data,
as it is not affected by extreme values unlike the range or variance.
\section{Boxplots}
\subsection{Five Number Summary}
The five number summary is set of measurements that indicates the
\begin{itemize}
    \item minimum value
    \item 25\% quartile
    \item median
    \item 75\% quartile
    \item maximum value
\end{itemize}

\subsection{Outliers}
Outliers are extreme observations that fall outside some
interval defined either by quantiles (above 95\% or below 5\% quantiles) or in terms of the Empirical rule
(outside two standard deviations from the mean).
They should be investigated to determine if they are errors or naturally occurring
extreme values.

\subsection{Covariance and Correlation Coefficients}
Covariance is the measure of the linear correlation between variables.
For variables \(x\) and \(y\),
\begin{equation*}
    s_{xy} = \frac{\sum_{i = 1}^n \left( x_i - \overline{x} \right) \left( y_i - \overline{y} \right)}{n - 1}.
\end{equation*}
Note that when \(x = y\), the formula simplifies to the sample variance of \(x\).
The covariance has the following characteristics:
\begin{itemize}
    \item \(s_{xy} > 0\): As \(x\) increases, \(y\) also increases.
    \item \(s_{xy} < 0\): As \(x\) increases, \(y\) decreases.
    \item \(s_{xy} \approx 0\): No relationship between \(x\) and \(y\).
\end{itemize}
Although the covariance is a useful tool to measure relationships, it is
only generalisable in terms of its sign. Thus, if we want to compare across
data sets, we need to use the correlation coefficient.
\begin{equation*}
    r_{xy} = \frac{s_{xy}}{s_x s_y}
\end{equation*}
The \textbf{correlation coefficient} is a measure of the strength of the relationship between the variables.
It is a scale-free and unitless measure bounded between \(-1\) and \(1\) and
has the same characteristics as the covariance.

Note that a correlation coefficient of \(0\)
indicates \textbf{no linear relationship} between the variables, and not necessarily indicative of \textbf{no relationship}.
\section{Regression and Least Squares}
A regression or least squares line of
best fit provides both a graphical and numerical summary of the relationship between the variables.
A linear relationship between two variables \(x\) and \(y\) is defined as
\(
    y = a + b x.
    \)
The least squares best fit determines the coefficients \(a\) and \(b\) that minimise the sum of the squares of the residuals
(errors) between \(y\) and the line \(\hat{y} = a + b x\). Mathematically,
\(
    \min_{a,\: b} \sum_{i = 1}^n \left( y_i - \hat{y}\left( x_i \right) \right)^2.
    \)
The coefficients can be summarised by the formula
\begin{align*}
    b & = r \frac{s_y}{s_x} = \frac{s_{xy}}{s_x^2} \\
    a & = \overline{y} - b \overline{x}.
\end{align*}


    \section{Events and Probability}
    \subsection{Event}
    Set of outcomes from an experiment.
    \subsection{Sample Space}
    Set of all possible outcomes \(\Omega\).
    \subsection{Intersection}
    Outcomes occur in both \(A\) and \(B\)
    \begin{equation*}
        A \cap B \quad\quad \text{or} \quad\quad AB
    \end{equation*}
    \subsection{Disjoint}
    No common outcomes, \(AB = \varnothing\)
    \begin{equation*}
        \Pr{\left( AB \right)} = \Pr{\left( A \,\vert\, B \right)} = 0
    \end{equation*}
    \subsection{Union}
    Set of outcomes in either \(A\) or \(B\)
    \begin{equation*}
        A \cup B
    \end{equation*}
    \subsection{Complement}
    Set of all outcomes not in \(A\), but in \(\Omega \)
    \begin{align*}
        A\overline{A}       & = \varnothing \\
        A \cup \overline{A} & = \Omega
    \end{align*}
    \subsection{Subset}
    \(A\) is a (non-strict) subset of \(B\) if all elements in \(A\) are also in \(B\) --- \(A \subset B\).
    \begin{equation*}
        AB = A \quad\quad \text{and} \quad\quad A \cup B = B
    \end{equation*}
    \begin{equation*}
        \forall A:A\subset \Omega \land \varnothing \subset A
    \end{equation*}
    \begin{align*}
        \Pr{\left( A \right)}             & \leq \Pr{\left( B \right)}                            \\
        \Pr{\left( B \,\vert\, A \right)} & = 1                                                   \\
        \Pr{\left( A \,\vert\, B \right)} & = \frac{\Pr{\left( A \right)}}{\Pr{\left( B \right)}}
    \end{align*}
    \subsection{Identities}
    \begin{align*}
        A \left( BC \right)            & = \left( AB \right) C                             \\
        A \cup \left( B \cup C \right) & = \left( A \cup B \right) \cup C                  \\
        A \left(B \cup C\right)        & = AB \cup AC                                      \\
        A \cup BC                      & = \left( A \cup B \right) \left( A \cup C \right)
    \end{align*}
    \subsection{Probability}
    Measure of the likeliness of an event occurring
    \begin{equation*}
        \Pr{\left( A \right)} \quad\quad \text{or} \quad\quad \mathrm{P}\left( A \right)
    \end{equation*}
    \begin{equation*}
        0 \leq \Pr{\left( A \right)} \leq 1
    \end{equation*}
    where a probability of 0 never happens, and 1 always happens.
    \begin{align*}
        \Pr{\left( \Omega \right)}       & = 1                         \\
        \Pr{\left( \overline{A} \right)} & = 1 - \Pr{\left( A \right)}
    \end{align*}
    \subsection{Multiplication Rule}
    For independent events \(A\) and \(B\)
    \begin{equation*}
        \Pr{\left( AB \right)} = \Pr{\left( A \right)} \Pr{\left( B \right)}.
    \end{equation*}
    For dependent events \(A\) and \(B\)
    \begin{equation*}
        \Pr{\left( AB \right)} = \Pr{\left( A \,\vert \, B \right)} \Pr{\left( B \right)}
    \end{equation*}
    \subsection{Addition Rule}
    For independent \(A\) and \(B\)
    \begin{equation*}
        \Pr{\left( A \cup B \right)} = \Pr{\left( A \right)} + \Pr{\left( B \right)} - \Pr{\left( AB \right)}.
    \end{equation*}
    If \(AB = \varnothing \), then \(\Pr{\left( AB \right)} = 0\), so that \(\Pr{\left( A \cup B \right)} = \Pr{\left( A \right)} + \Pr{\left( B \right)}\).
    \subsection{De Morgan's Laws}
    \begin{align*}
        \overline{A \cup B} & = \overline{A} \ \overline{B}     \\
        \overline{AB}       & = \overline{A} \cup \overline{B}.
    \end{align*}
    \begin{align*}
        \Pr{\left( A \cup B \right)} & = 1 - \Pr{\left( \overline{A} \ \overline{B} \right)}    \\
        \Pr{\left( AB \right)}       & = 1 - \Pr{\left( \overline{A} \cup \overline{B} \right)}
    \end{align*} 
    \subsection{Bayes' Theorem}
    \begin{equation*}
        \Pr{\left( A \,\vert\, B \right)} = \frac{\Pr{\left( B \,\vert\, A \right)}\Pr{\left( A \right)}}{\Pr{\left( B \right)}}
    \end{equation*}
    \section{Random Variables}
        Measurable variable whose value holds some uncertainty.
        An event is when a random variable assumes a certain value or range of values.
        \subsection{Probability Distribution}
        The probability distribution of a random variable \(X\) is a function that links all outcomes \(x \in \Omega\)
        to the probability that they will occur \(\Pr{\left( X = x \right)}\).
        \subsection{Probability Mass Function}
        \begin{equation*}
            \Pr{\left( X = x \right)} = p_x
        \end{equation*}
        \subsection{Probability Density Function}
        \begin{equation*}
            \Pr{\left( x_1 \leq X \leq x_2 \right)} = \int_{x_1}^{x_2} f\left( x \right) \odif{x}
        \end{equation*}
        \subsection{Cumulative Distribution Function}
        Probability that a random variable is
        less than or equal to a particular realisation \(x\).

        \(F\left( x \right)\) is a valid CDF if:
        \begin{enumerate}
            \item \(F\) is monotonically increasing and continuous
            \item \(\lim_{x \to -\infty} F\left( x \right) = 0\)
            \item \(\lim_{x \to \infty} F\left( x \right) = 1\)
        \end{enumerate}
        \begin{equation*}
            \odv{F\left( x \right)}{x} = \odv{}{x} \int_{-\infty}^x f\left( u \right) \odif{u} = f\left( x \right)
        \end{equation*}
        \subsection{Complementary CDF (Survival Function)}
        \begin{equation*}
            \Pr{\left( X > x \right)} = 1 - \Pr{\left( X \leq x \right)} = 1 - F\left( x \right)
        \end{equation*}
        \subsection{\texorpdfstring{\(p\)}{p}-Quantiles}
        \begin{equation*}
            F\left( x \right) = \int_{-\infty}^x f\left( u \right) \odif{u} = p
        \end{equation*}
        \subsection{Special Quantiles}
        \begin{align*}
            \text{Lower quartile \(q_1\):}  &  &  & p = \frac{1}{4} \\
            \text{Median \(m\):}            &  &  & p = \frac{1}{2} \\
            \text{Upper quartile \(q_2\):}  &  &  & p = \frac{3}{4} \\
            \text{Interquartile range IQR:} &  &  & q_2 - q_1
        \end{align*}
        \subsection{Quantile Function}
        \begin{equation*}
            x = F^{-1}\left( p \right) = Q\left( p \right)
        \end{equation*}
        \subsection{Expectation (Mean)}
        Expected value given an infinite number of observations. For \(a < c < b\):
        \begin{equation*}
            \E{\left(X\right)} = \begin{aligned}[t]
                 & -\int_{a}^c F\left( x \right) \odif{x}                     \\
                 & + \int_c^b \left(1 - F\left( x \right)\right) \odif{x} + c
            \end{aligned}
        \end{equation*}
        \subsection{Variance}
        Measure of spread of the distribution (average squared distance of each value from the mean).
        \begin{equation*}
            \Var{\left( X \right)} = \sigma^2 = \E{\left( X^2 \right)} - \E{\left( X \right)}^2
        \end{equation*}
        \subsection{Standard Deviation}
        \begin{equation*}
            \sigma = \sqrt{\Var{\left( X \right)}}
        \end{equation*}
\end{multicols}
\begin{table}[H]
    \centering
    \begin{tabular}{c c c c c c}
        \toprule
        \textbf{Distribution}                                      & \textbf{Restrictions}                                  & \textbf{PMF}                                              & \textbf{CDF}                                                             & \(\E{\left( X \right)}\)            & \(\Var{\left( X \right)}\)                    \\
        \midrule
        \(X \sim \operatorname{Uniform}{\left( a,\: b \right)}\)   & \(x \in \left\{ a, \dots, b \right\}\)                 & \(\frac{1}{b - a + 1}\)                                   & \(\frac{x - a + 1}{b - a + 1}\)                                          & \(\frac{a + b}{2}\)                 & \(\frac{\left( b - a + 1 \right)^2 - 1}{12}\) \\
        \(X \sim \operatorname{Bernoulli}{\left( p \right)}\)      & \(p \in \interval{0}{1}, x \in \left\{ 0, 1 \right\}\) & \(p^x \left( 1 - p \right)^{1 - x}\)                      & \(1 - p\)                                                                & \(p\)                               & \(p \left( 1 - p \right)\)                    \\
        \(X \sim \operatorname{Binomial}{\left( n,\: p \right)}\)  & \(x \in \left\{ 0, \dots, n \right\}\)                 & \(\binom{n}{x} p^x \left( 1 - p \right)^{n - x}\)         & \(\sum_{u = 0}^x \binom{n}{u} p^u \left( 1 - p \right)^{n - u}\)         & \(np\)                              & \(np\left( 1 - p \right)\)                    \\
        \( N \sim \operatorname{Poisson}{\left( \lambda \right)}\) & \(n \geq 0\)                                           & \(\frac{\lambda^n e^{-\lambda}}{n!}\)                     & \(e^{-\lambda} \sum_{u = 0}^n \frac{\lambda^u}{u!}\)                     & \(\lambda\)                         & \(\lambda\)                                   \\
        \bottomrule
    \end{tabular}
    \caption{Discrete probability distributions.} % \label{}
\end{table}
\begin{table}[H]
    \centering
    \begin{tabular}{c c c c c c}
        \toprule
        \textbf{Distribution}                                       & \textbf{Restrictions}                  & \textbf{PDF}                                                                         & \textbf{CDF}                                                                            & \(\E{\left( X \right)}\) & \(\Var{\left( X \right)}\)            \\
        \midrule
        \(X \sim \operatorname{Uniform}{\left( a,\: b \right)}\)    & \(a < x < b\)                          & \(\frac{1}{b - a}\)                                                                  & \(\frac{x - a}{b - a}\)                                                                 & \(\frac{a + b}{2}\)      & \(\frac{\left( b - a \right)^2}{12}\) \\
        \(T \sim \operatorname{Exp}{\left( \eta \right)}\)          & \(t > 0\)                              & \(\eta e^{-\eta t}\)                                                                 & \(1 - e^{-\eta t}\)                                                                     & \(1/\eta\)               & \(1/\eta\)                            \\
        \(X \sim \operatorname{N}{\left( \mu,\: \sigma^2 \right)}\) & \(x \in \left\{ 0, \dots, n \right\}\) & \(\frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{\left( x - \mu \right)^2}{2 \sigma^2}}\) & \(\frac{1}{2} \left( 1 + \erf{\left( \frac{x - \mu}{\sigma \sqrt{2}} \right)} \right)\) & \(\mu\)                  & \(\sigma^2\)                          \\
        \bottomrule
    \end{tabular}
    \caption{Continuous probability distributions.} % \label{}
\end{table}
\begin{minipage}{126.1962963mm}
    \begin{table}[H]
        \centering
        \begin{tabular}{c c c }
            \toprule
                                                     & \textbf{Discrete}                              & \textbf{Continuous}                                    \\
            \midrule
            Valid probabilities                      & \(0 \leq p_x \leq 1\)                          & \(f(x) \geq 0\)                                        \\
            Cumulative probability                   & \(\sum_{u \leq x} p_u\)                        & \(\int_{-\infty}^{x} f(u) \odif{u}\)                   \\
            \(\E{\left( X \right)}\)                 & \(\sum_{\Omega} xp_x\)                         & \(\int_{\Omega} xf(x)\odif{x}\)                        \\
            \(\E{\left( g\left( X \right) \right)}\) & \(\sum_{\Omega} g\left( x \right)p_x\)         & \(\int_{\Omega} g\left( x \right)f(x)\odif{x}\)        \\
            \(\Var{\left( X \right)}\)               & \(\sum_{\Omega} \left( x - \mu \right)^2 p_x\) & \(\int_{\Omega} \left( x - \mu \right)^2f(x)\odif{x}\) \\
            \bottomrule
        \end{tabular}
        \caption{Probability rules for univariate \(X\).} % \label{}
    \end{table}
\end{minipage}
\end{document}
